####

#### Example Results Files
# exampleResults/Covid-P001_NFSamples.csv @@@ Contains library_id and s3FastqR1 and s3FastqR2 that are fed into NextFlow pipeline. This file is generated by fetchAndupload.sh
# exampleResults/NFResults.csv @@@ Contains information obtained from Nextflow viralrecon run: Coverage, depth, sequence and intermediat file location
# exampleResults/NextCladeSequences_output.csv @@@ Contains infromation obtained from NextClade Analysis including quality, mutations, and clade.

require(aws.s3)
require(tidyr)
require(tidyverse)
require(optparse)
require(stringr)
require(dplyr)


### GISAID Uploader
args = commandArgs(trailingOnly=TRUE)

if(is.na(args[1])){
  print("You need to specify a result file containing data needes by gisaid in the specified coding")
  print("Metadata file needs to include library_id, at least")
}

gisaidProcess<-function(ResultsFile)
{
  GisaidSubmitRoot=gsub(".csv","",ResultsFile)
  GisaidSubmitFasta=paste(GisaidSubmitRoot,"_gisaid.fasta",sep="")
  GisaidFastaFilename=basename(GisaidSubmitFasta)
  GisaidSubmitCsv=paste(GisaidSubmitRoot,"_gisaid.csv",sep="")

  bucket <- Sys.getenv("s3Bucket")

  outputDF <- data.frame(matrix(ncol = 30, nrow = 0))
  y<-c("submitter","fn","covv_virus_name","covv_type","covv_passage","covv_collection_date",
       "covv_location","covv_add_location","covv_host","covv_add_host_info","covv_gender",
       "covv_patient_age","covv_patient_status","covv_specimen","covv_sampling_strategy","covv_outbreak","covv_last_vaccinated",
       "covv_treatment","covv_seq_technology","covv_assembly_method","covv_coverage","covv_orig_lab","covv_orig_lab_addr",
       "covv_provider_sample_id","covv_subm_lab","covv_subm_lab_addr","covv_subm_sample_id","covv_authors","covv_comment","comment_type")


  x <- c("Submitter", "FASTA filename", "Virus name","Type",
         "Passage details/history","Collection date","Location",
         "Additional location information","Host","Additional host information",
         "Gender","Patient age","Patient status","Specimen source","Sampling Strategy",
         "Outbreak","Last vaccinated","Treatment","Sequencing technology",
         "Assembly method","Coverage","Originating lab","Address",
         "Sample ID given by the sample provider","Submitting lab",
         "Address","Sample ID given by the submitting laboratory",
         "Authors","","")
  colnames(outputDF) <- y
  # outputDF[1,]<-x

  DF<-read.csv(ResultsFile,fileEncoding = "UTF-8",sep=";")

  gisaidType<-"betacoronavirus"
  gisaidSequencingTechnology<-"Illumina/MiSeq"
  gisaidAssemblyMethod<-"Viralrecon/iVar"

  ### The following avoids adding any author as a local author for sequences from Microbiologia_HUGTiP. In these samples authorship is agreed upon and pre-defined.
  if( unique(DF$StudyID) == "Microbiologia_HUGTiP" | unique(DF$StudyID)=="Micro_HUGTiP"){
    gisaidLocalAuthors<-""
    gisaidSubmittingLab<-"Can Ruti SARS-CoV-2 Sequencing Hub (HUGTiP/IrsiCaixa/IGTP)"
    gisaidSubmittingLabAddress<-"Hospital Universitari Germans Trias i Pujol, 2a planta maternal, Ctra Canyet s/n, 08916 Badalona, Catalonia, Spain."
    gisaidSubmitter<-"mnoguera"
  }else{
    gisaidLocalAuthors<-paste("Marc Noguera-Julian","Mariona Parera","Maria Casadellà", "Pilar Armengol", "Francesc Catala-Moll", "Roger Paredes", "Bonaventura Clotet",sep=", ")
    gisaidLocalAuthors<-iconv(gisaidLocalAuthors,from="UTF-8",to="UTF-8")
    gisaidSubmittingLab<-"IrsiCaixa"
    gisaidSubmittingLabAddress<-"Fundació irsiCaixa. Hospital Universitari Germans Trias i Pujol(HUGTiP), 2a planta, maternal Ctra Canyet s/n, Badalona"
    gisaidSubmitter<-"mnoguera"
  }

  # gisaidSubmittingLabAddress<-iconv(gisaidSubmittingLabAddress,from="UTF-8",to="UTF-8")


  system(paste("rm",GisaidSubmitFasta))
  DF<-DF[! is.na(DF$qc.overallStatus),]
  DF$qc.overallStatus<-as.factor(DF$qc.overallStatus)
  DF[is.na(DF$treatment),"treatment"]<-"Unknown"
  DF[is.na(DF$host_comment),"host_comment"]<-"--"
  DF[is.na(DF$AnalysisComments),"AnalysisComments"]<-"NA"
  if(is.null(DF$location_comment)){
    DF$location_comment<-NA
  }
  if(is.null(DF$patient_status)){
    DF$patient_status<-NA
  }
  for (i in 1:nrow(DF)){
    ### Create a single file for sequences that pass the publishable criteria
    if((as.character(DF[i,"qc.overallStatus"]) %in% c("good","mediocre","bad")) & (DF[i,"PercCov"]>=80) & ( ! is.na(DF[i,"collection_date"])) & ( ! str_detect(DF[i,"AnalysisComments"], regex("control", ignore_case = TRUE)))){
      print(DF[i,"qc.overallStatus"])
      if(unique(DF$StudyID) == "Microbiologia_HUGTiP"){
        idHeader<-"hCoV-19/Spain/CT-HUGTiP"
      }else if (grepl("CRESA",unique(DF$StudyID))){
        idHeader<-paste("hCoV-19/Spain/CT-",as.character(DF[i,"GISAID_label"]),"-",as.character(DF[i,"sample_id"]),"-IrsiCaixa",sep="")
      }else{
        idHeader<-"hCoV-19/Spain/CT-IrsiCaixa"
      }
      print(idHeader)
      # idHeader<-ifelse(unique(DF$StudyID) == "Microbiologia_HUGTiP","hCoV-19/Spain/CT-HUGTiP","hCoV-19/Spain/CT-IrsiCaixa")
      virus_name<-paste(idHeader,DF[i,"library_id"],"/",format(as.Date(DF[i,"collection_date"]),"%Y"),sep="")
      write(paste(">",virus_name,"\n",DF[i,"FastqSequence"],sep=""),file=GisaidSubmitFasta,append=T)
      outputDF[i,"submitter"]<-gisaidSubmitter
      outputDF[i,"fn"]<-GisaidFastaFilename
      outputDF[i,"covv_virus_name"]<-virus_name
      outputDF[i,"covv_type"]<-gisaidType
      outputDF[i,"covv_passage"]<-as.character(DF[i,"passage_details"])
      outputDF[i,"covv_collection_date"]<-as.character(as.Date(DF[i,"collection_date"],"%Y-%m-%d"))
      ### Note that accents and others are remove from GISAID metadata. Consider use stringi::stri_trans_general
      ###stringr::str_to_title(textclean::replace_non_ascii(DF[i,"location"]))
      outputDF[i,"covv_location"]<-as.character(paste("Europe","Spain","Catalunya",stringr::str_to_title(textclean::replace_non_ascii(DF[i,"location"])),sep=" / "))
      outputDF[i,"covv_add_location"]<-ifelse(is.na(DF[i,"location_comment"]),"",DF[i,"location_comment"])
      outputDF[i,"covv_host"]<-DF[i,"host"]
      outputDF[i,"covv_add_host_info"]<-ifelse(is.na(DF[i,"host_comment"]),"",DF[i,"host_comment"])
      outputDF[i,"covv_gender"]<-DF[i,"gender"]
      outputDF[i,"covv_patient_age"]<-DF[i,"age"]
      outputDF[i,"covv_patient_status"]<-ifelse(is.na(DF[i,"patient_status"]),"Unknown",DF[i,"patient_status"]) ### Use Unknown
      outputDF[i,"covv_specimen"]<-DF[i,"source"]
      outputDF[i,"covv_sampling_strategy"]<-ifelse(is.na(DF[i,"sequencing_strategy"]),"Unknown",DF[i,"sequencing_strategy"])
      outputDF[i,"covv_outbreak"]<-DF[i,"outbreak"]
      outputDF[i,"covv_last_vaccinated"]<-ifelse(is.na(DF[i,"vaccinated"]),"",DF[i,"vaccinated"])
      outputDF[i,"covv_treatment"]<-ifelse(is.na(DF[i,"treatment"]),"",DF[i,"treatment"])
      outputDF[i,"covv_seq_technology"]<-gisaidSequencingTechnology
      outputDF[i,"covv_assembly_method"]<-gisaidAssemblyMethod
      outputDF[i,"covv_coverage"]<-DF[i,"DepthOfCov"]
      outputDF[i,"covv_orig_lab"]<-DF[i,"OriginatingLab"]
      outputDF[i,"covv_orig_lab_addr"]<-DF[i,"OriginatingLabAddress"]
      outputDF[i,"covv_provider_sample_id"]<-DF[i,"sample_id"]
      outputDF[i,"covv_subm_lab"]<-gisaidSubmittingLab
      outputDF[i,"covv_subm_lab_addr"]<-gisaidSubmittingLabAddress
      outputDF[i,"covv_subm_sample_id"]<-DF[i,"library_id"]
      outputDF[i,"covv_authors"]<-paste(gisaidLocalAuthors,DF[i,"OriginatingLabAuthors"])
      outputDF[i,"covv_comment"]<-""
      outputDF[i,"comment_type"]<-""
    }
  }
  outputDF<-outputDF[! rowSums(is.na(outputDF))==ncol(outputDF),]
  write.csv(outputDF,file=GisaidSubmitCsv,row.names = F,fileEncoding = "UTF-8")
  #Upload to s3
  myFileName<-basename(GisaidSubmitCsv)
  put_object(GisaidSubmitCsv,paste("Runs/",projectString,myFileName,sep=""),bucket,multipart=T)
  put_object(GisaidSubmitFasta,paste("Runs/",projectString,GisaidFastaFilename,sep=""),bucket,multipart=T)
  DF$passage_details
}

args = commandArgs(trailingOnly=TRUE)

if(is.na(args[1])){
  print("You need to specify NFSamplesFile NextCladeOutputFile MetadataFile all in csv format.")
  print("Metadata file needs to include library_id, at least")
}

projectString<-args[1]
# projectString<-"2021-06-08_Covid-M018_270614344/"
projectID<-strsplit(projectString,"_")
projectID<-projectID[[1]][2]
MetadataFile=args[2]
# MetadataFile="~/Downloads/metadata_to_fetch_run_M018.csv"
# MetadataFile="/tmp/metadata_to_fetch_run_R014.csv"
bucket <- Sys.getenv("s3Bucket")
ResultDir <- Sys.getenv("ResultDir")
# ResultDir <- "~/Downloads"

print(bucket)
### Read Viralrecon output from S3.
print("Reading NextFlow/Viralrecon output")
print(paste("Runs/",projectString,sep=""))
s3NFOutput <- bucket %>%
  aws.s3::get_bucket_df(prefix = paste("Runs/",projectString,sep=""),max=Inf) %>%
  filter(str_detect(Key, projectString)) %>%
  filter(str_detect(Key, "NFResults.csv" ))
NFSamplesDF<-s3NFOutput %>%
  pull(Key) %>%
  aws.s3::s3read_using(object=.,
                       FUN=readr::read_csv,
                       col_type=cols(),
                       bucket=bucket)

print(nrow(NFSamplesDF))

print("Reading Pangolin Output from s3")
#### Read Pangoling from S3
s3PGOutput <- bucket %>%
  aws.s3::get_bucket_df(prefix = paste("Runs/",projectString,sep="")) %>%
  filter(str_detect(Key, projectString)) %>%
  filter(str_detect(Key, "Pangolin_output.csv" ))
PGOutputDF<-s3PGOutput%>%
  pull(Key) %>%
  aws.s3::s3read_using(object = .,
                       FUN = readr::read_csv,
                       col_types = cols(),
                       bucket = bucket,) %>%
  dplyr::rename(library_id = taxon)

print(nrow(PGOutputDF))

print("Reading Nextclade outputfrom s3")
### Read NextClade from S3.
s3NCOutput <-  bucket %>%
  aws.s3::get_bucket_df(prefix = paste("Runs/",projectString,sep="")) %>%
  filter(str_detect(Key, projectString)) %>%
  filter(str_detect(Key, "NextCladeSequences_output.csv" ))
NCOutputDF<-s3NCOutput%>%
  pull(Key) %>%
  aws.s3::s3read_using(object = .,
                       FUN = readr::read_delim,
                       delim = ";",
                       col_types = cols(),
                       bucket = bucket) %>%
  dplyr::rename(library_id=seqName)
print(nrow(NCOutputDF))


#### Merge NextFlow and NextClade results with sample Data

NFNCDF<-merge(NFSamplesDF,NCOutputDF,all.x=TRUE,by="library_id")
colnames(NFNCDF)
NFNCPGDF<-merge(NFNCDF,PGOutputDF,all.x=T,by="library_id")

if(grepl(c(".xlsx",".xls"),MetadataFile)){
  MetadataDF<-xlsx::read.xlsx(MetadataFile,encoding = "UTF-8",sheetIndex=2)
  colnames(MetadataDF)
  colnames(MetadataDF)<-MetadataDF[1,]
  MetadataDF<-MetadataDF[-1,]
  colnames(MetadataDF)
  MetadataDF<-MetadataDF[! rowSums(is.na(MetadataDF))==ncol(MetadataDF),]
  LibraryIDDF<-xlsx::read.xlsx(MetadataFile,encoding = "UTF-8",sheetIndex=3)
  colnames(LibraryIDDF)
  colnames(LibraryIDDF)<-LibraryIDDF[1,]
  LibraryIDDF<-LibraryIDDF[-1,]
  colnames(LibraryIDDF)
  LibraryIDDF<-LibraryIDDF[! rowSums(is.na(LibraryIDDF))==ncol(LibraryIDDF),]
  LibraryIDDF[,"design_date"]<-as.character((openxlsx::convertToDate(LibraryIDDF[,"design_date"])))
  LibraryIDDF[,"run_date"]<-as.character((openxlsx::convertToDate(LibraryIDDF[,"run_date"])))

  ### re-polish sample_id and secondary_id
  MetadataDF$sample_id<-gsub(" ","",MetadataDF$sample_id)
  MetadataDF$secondary_id<-gsub(" ","",MetadataDF$secondary_id)
  LibraryIDDF$sample_id<-gsub(" ","",LibraryIDDF$sample_id)
  LibraryIDDF$secondary_id<-gsub(" ","",LibraryIDDF$secondary_id)
  MetadataDF<-merge(MetadataDF,LibraryIDDF,by=c("sample_id","secondary_id"))
  ### Polish encoding of accents and other shit
  Encoding(MetadataDF$OriginatingLab[2])
  MetadataDF<-MetadataDF[! rowSums(is.na(MetadataDF))==ncol(MetadataDF),]
  MetadataDF<-MetadataDF[,! colSums(is.na(MetadataDF))==nrow(MetadataDF)]
  MetadataDF[,"collection_date"]<-as.character((openxlsx::convertToDate(MetadataDF[,"collection_date"]))) ### Date format forced in template screw import up

  MetadataDF$collection_date
}else{
  ### Reading from csv should be much easier
  MetadataDF<-read.delim(MetadataFile,sep=";") %>%
    dplyr::select(-library_id)%>%
    dplyr::rename(library_id=fastq_id)
}

# MetadataDF$OriginatingLab<-iconv(MetadataDF$OriginatingLab,from="UTF-8",to="TRANSLIT")
# MetadataDF$OriginatingLabAddress<-iconv(MetadataDF$OriginatingLabAddress,from="UTF-8",to="UTF-8")
# MetadataDF$OriginatingLabAuthors<-iconv(MetadataDF$OriginatingLabAuthors,from="UTF-8",to="UTF-8")
# MetadataDF$AnalysisComments<-iconv(MetadataDF$AnalysisComments,from="UTF-8",to="UTF-8")
# MetadataDF$RunProjectID<-iconv(MetadataDF$RunProjectID,from="UTF-8",to="UTF-8")

MetadataNFNCDF<-merge(MetadataDF,NFNCPGDF,by.x="library_id",by.y="library_id",all=T)
Encoding(MetadataNFNCDF$OriginatingLab[2])
colnames(MetadataNFNCDF)[1]<-"library_id"
MetadataDF$OriginatingLab
MetadataNFNCDF$RunProjectID
print(paste("ResultDir is:",ResultDir))
#### Producte reports for internal use
#### This will be project-specific and contain comprehensive pipeline information for further processing/or not
MetadataNFNCDF$StudyID<-as.factor(MetadataNFNCDF$StudyID)
levels(MetadataNFNCDF$StudyID)
for(study in levels(MetadataNFNCDF$StudyID)){
  mySubDF<-subset.data.frame(MetadataNFNCDF,StudyID==study)
  #Use ; for FS because mutation list have ","
  write.table(mySubDF,file=paste(ResultDir,projectID,"_",study,".csv",sep=""),row.names = F,fileEncoding = "UTF-8" ,sep=";")
  #Upload to s3
  myFileObject<-paste(ResultDir,projectID,"_",study,".csv",sep="")
  put_object(myFileObject,paste("Runs/",projectString,projectID,"_",study,".csv",sep=""),bucket,multipart=T)
  ## Generate GISAID files
  gisaidProcess(paste(ResultDir,projectID,"_",study,".csv",sep=""))

  #Generate XLS files
  xlsx::write.xlsx(mySubDF,file=paste(ResultDir,projectID,"_",study,".xlsx",sep=""),row.names = F)
  #Upload to s3
  myFileObject<-paste(ResultDir,projectID,"_",study,".xlsx",sep="")
  put_object(myFileObject,paste("Runs/",projectString,projectID,"_",study,".xlsx",sep=""),bucket,multipart=T)
}


### Keep track of study specific fasta files. These may be handy for internal result reporting.
for(study in levels(MetadataNFNCDF$StudyID)){
  system(paste("rm",paste(ResultDir,projectID,"_",study,".fasta",sep="")))
  mySubDF<-subset.data.frame(MetadataNFNCDF,StudyID==study)
  # fileConn<-file(paste("/Users/mnoguera/Downloads/",projectID,"_",study,".fasta",sep=""),)
  for (i in 1:nrow(mySubDF)){
    print(i)
    # writeLines(paste(">",mySubDF[i,"library_id"],"\n",mySubDF[i,"FastqSequence"],"\n"),fileConn)
    write(paste(">",mySubDF[i,"library_id"],"\n",mySubDF[i,"FastqSequence"]),file=paste(ResultDir,projectID,"_",study,".fasta",sep=""),append=T)
	#Upload to s3
  	myFileObject<-paste(ResultDir,projectID,"_",study,".fasta",sep="")
  	put_object(myFileObject,paste("Runs/",projectString,projectID,"_",study,".fasta",sep=""),bucket,multipart=T)
  }
  # close(fileConn)
}

print(paste("ResultDir is:",ResultDir))
write.table(MetadataNFNCDF,file=paste(ResultDir,projectID,".csv",sep=""),row.names = F,fileEncoding = "UTF-8" ,sep=";")
myFileObject<-paste(ResultDir,projectID,".csv",sep="")
put_object(myFileObject,paste("Runs/",projectString,projectID,".csv",sep=""),bucket,multipart=T)

#### Produce files for GISAID batch upload

# put_object(myUpdateAggregated,paste("Runs/UpdatedData/Updated_",Sys.Date(),"_updated.csv",sep=""),bucket,multipart=T)

